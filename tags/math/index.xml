<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>math on Miles Lucas</title><link>/tags/math/</link><description>Recent content in math on Miles Lucas</description><generator>Hugo -- gohugo.io</generator><language>en</language><copyright>&amp;copy; 2021 &lt;a href="https://mileslucas.com/">Miles Lucas&lt;/a></copyright><lastBuildDate>Mon, 19 Apr 2021 00:06:19 -0500</lastBuildDate><atom:link href="/tags/math/index.xml" rel="self" type="application/rss+xml"/><item><title>Developing a Single-Pass Weighted LogSumExp Function</title><link>/posts/weighted-logsumexp/</link><pubDate>Mon, 19 Apr 2021 00:06:19 -0500</pubDate><guid>/posts/weighted-logsumexp/</guid><description>Recently I&amp;rsquo;ve been thinking about the LogSumExp trick since it is used in the integration step of nested sampling. I won&amp;rsquo;t go over too much of the math here, but the reason this trick exists is to greatly increase the numerical stability of the operation
$$ \log \sum_i \exp x_i $$
via the identity
$$ a + \log \sum_i \exp\left(x_i - a\right) $$
Naive implementations In Julia we can implement a naive logsumexp with
logsumexp_naive(X) = log(sum(exp, X)) let&amp;rsquo;s test the numerical accuracy against Julia&amp;rsquo;s BigFloat for some very large numbers</description></item></channel></rss>